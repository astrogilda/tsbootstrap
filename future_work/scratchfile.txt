base classes:
Stationary
moving
NonOverlappingSubseries
Circular


tapered -- block
adaptive -- block
Markov -- [whole] and block
Biascorrected -- block and whole
residual/non-parametric (ar-x, arima, var, sarima) -- block and whole
Sieve (ar-x, arima, var, sarima, garch) -- [block **TO DO*] and whole
Fractional -- whole and block
Distributional -- weibull, gamma, Exponential, gamma, bayesian

#TODO:
Spectral -- needs significant reworking; need to properly install that library
Multi-tapered
HAC -- block and whole

(pair -- same as moving with block_length=2) TODO: test this





# TODO: use TypeVar to allow for different types of data (e.g. numpy arrays, pandas dataframes, lists, etc.)
# TODO: add option for block length to be a fraction of the data length
# TODO: use Type to indicate the type of the data (even classes), instead of directly reference the instance
# TODO: add option for multiprocessing using mp.pool in _iter_test_masks and _generate_samples
# TODO: add an option for VECM (vector error correction model) bootstrap
# TODO: write a data abstraction layer that converts data to a common format (e.g. numpy arrays) and back. also convert 1d to 2d arrays, and check for dimensionality and type consistency and convert if necessary.
# TODO: test vs out-of-bootstrap samples; should we split in the abstract layer itself and only pass the training set to the bootstrap generator?
# TODO: __init__ files for all submodules
# TODO: explain how _iter_test_mask is only kept for backwards compatibility and is not used in the bootstrap generator
# TODO: add option for block_length to be None, in which case it is set to the square root of the data length
# TODO: add option for block_length to be a list of integers, in which case it is used as the block length for each bootstrap sample
# TODO: Hierarchical Archimedean Copula





Here are some situations where bias-corrected bootstrap might be appropriate:

Small Sample Sizes: Bootstrap methods, in general, are particularly useful when the sample size is small, and assumptions of asymptotic methods might not hold. When sample size is small, the bootstrap distribution might be more biased, making bias correction potentially beneficial.

Skewed Data or Non-Normal Distribution: If the original data are skewed or not normally distributed, the mean of the bootstrap samples might be biased. In this case, bias-corrected bootstrap could be helpful.

Complex Statistics: When dealing with more complex statistics where bias could be a concern, bias-corrected bootstrap might be a good approach.

Here are some situations where bias-corrected bootstrap might not be appropriate or beneficial:

Large Sample Sizes: With large sample sizes, the bootstrap distribution tends to be less biased, and bias correction might not be necessary or beneficial.

Independence Assumptions Violated: If the data are not independent and identically distributed (i.i.d.), bootstrapping might not be appropriate at all. This includes time series data, clustered data, etc. Note that there are modifications of the bootstrap method, such as block bootstrap, that can handle some forms of dependence, but these methods also have their assumptions and limitations.

Increased Variability: While bias-corrected bootstrap can reduce bias, it might also increase variability. If reducing variability is more important than reducing bias in a particular analysis, bias-corrected bootstrap might not be the best approach.
